{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YNtpc4I6DtHt","executionInfo":{"status":"ok","timestamp":1734835532895,"user_tz":-480,"elapsed":3350,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"3c2d4b1d-662c-47be-ce31-6413e99d030b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huizRK-Cy0KG","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1734833544632,"user_tz":-480,"elapsed":1105041,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"c222b408-a305-48da-d915-4c508b0408c3"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/IRTM_final_project/data/posts/1220.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ca55de4a14c9>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mvectors_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/IRTM_final_project/vectors/1220_vector.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcluster_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cluster_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/IRTM_final_project/cluster_result/HAC_0.1_Kmeans_complete.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtexts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/IRTM_final_project/data/posts/1220.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dictionary Loaded: {len(dictionary)} terms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-ca55de4a14c9>\u001b[0m in \u001b[0;36mload_texts\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/IRTM_final_project/data/posts/1220.csv'"]}],"source":["import pandas as pd\n","import numpy as np\n","import ast\n","import csv\n","\n","def load_dictionary(file_path):\n","    term_dict = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            term, term_id = line.strip().split(': ')\n","            term_dict[term] = int(term_id)\n","    return term_dict\n","\n","def load_vectors(file_path):\n","    df = pd.read_csv(file_path, encoding='utf-8')\n","    df['vector'] = df['vector'].apply(lambda x: np.array(eval(x), dtype=np.float64))\n","    return df\n","\n","def load_cluster_results(file_path):\n","    cluster_df = pd.read_csv(file_path, encoding='utf-8')\n","    cluster_df['members'] = cluster_df['members'].apply(lambda x: x.split(','))\n","    return cluster_df\n","\n","def load_texts(file_path):\n","    df = pd.read_csv(file_path, encoding='utf-8')\n","    df = df[['id', 'text']]\n","    return df\n","\n","dictionary = load_dictionary('/content/drive/MyDrive/IRTM_final_project/vectors/dictionary.txt')\n","vectors_df = load_vectors('/content/drive/MyDrive/IRTM_final_project/vectors/1220_vector.csv')\n","\n","\n"]},{"cell_type":"code","source":["texts_df = load_texts('/content/drive/MyDrive/IRTM_final_project/data/posts/1220.csv')\n","\n","print(f\"Dictionary Loaded: {len(dictionary)} terms\")\n","print(f\"Vectors Loaded: {vectors_df.shape[0]} articles\")\n","print(f\"Texts Loaded: {texts_df.shape[0]} articles\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLF8nYSnI3P6","executionInfo":{"status":"ok","timestamp":1734834085471,"user_tz":-480,"elapsed":2598,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"4e41a62f-0428-4b61-89b6-6b490cabddca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dictionary Loaded: 5000 terms\n","Vectors Loaded: 47967 articles\n","Cluster Results Loaded: 8 clusters\n","Texts Loaded: 80921 articles\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-ca55de4a14c9>:25: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path, encoding='utf-8')\n"]}]},{"cell_type":"code","source":["cluster_df = load_cluster_results('/content/drive/MyDrive/IRTM_final_project/cluster_result/kmeans.csv')\n","\n","print(f\"Cluster Results Loaded: {cluster_df.shape[0]} clusters\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGdYrq_yOGma","executionInfo":{"status":"ok","timestamp":1734835573513,"user_tz":-480,"elapsed":1002,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"24287304-0807-4c32-cba9-c711ba823aea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster Results Loaded: 8 clusters\n"]}]},{"cell_type":"code","source":["import re\n","import jieba\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","def preprocess_text(text):\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","\n","\n","    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n","    english_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n","\n","    chinese_tokens = jieba.lcut(text)\n","\n","    combined_tokens = english_tokens + chinese_tokens\n","    return ' '.join(combined_tokens)\n","\n","texts_df['processed_text'] = texts_df['text'].apply(preprocess_text)\n"],"metadata":{"id":"eG3oOnrwzCBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734835612019,"user_tz":-480,"elapsed":35055,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"c20cd63b-97b0-44f1-a08c-17c994165e23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["def extract_keywords_tfidf(df, cluster_df, vectors_df, dictionary, top_n=10):\n","    feature_names = list(dictionary.keys())\n","\n","    cluster_keywords = {}\n","\n","    for _, cluster in cluster_df.iterrows():\n","        cluster_name = cluster['cluster_name']\n","        cluster_members = cluster['members']\n","\n","        cluster_vectors = vectors_df[vectors_df['id'].isin(cluster_members)]['vector']\n","\n","        # 將這些 numpy.ndarray 堆疊成矩陣\n","        cluster_tfidf_matrix = np.vstack(cluster_vectors)\n","\n","        # 現在可以計算 TF-IDF 分數\n","        tfidf_scores = cluster_tfidf_matrix.sum(axis=0)\n","\n","        sorted_indices = tfidf_scores.argsort()[::-1]\n","\n","        top_keywords = [feature_names[i] for i in sorted_indices[:top_n]]\n","\n","        cluster_keywords[cluster_name] = top_keywords\n","\n","    return cluster_keywords\n","\n","cluster_keywords_tfidf = extract_keywords_tfidf(texts_df, cluster_df, vectors_df, dictionary)\n","print(cluster_keywords_tfidf)"],"metadata":{"id":"BCENZcZdzDsV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734835620389,"user_tz":-480,"elapsed":960,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"8f8d8e07-de0c-481c-ee78-79807b7c1507"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'cluster0': ['今天', 'the', '怎麼', '這個', 'day', '看到', '這樣', '演唱', '還是', '世界'], 'cluster1': ['真的', '看到', '這個', '今天', '不要', '可以', '時候', '這樣', '啊啊啊', '謝謝'], 'cluster2': ['可以', '我們', '大家', '一個', '台灣', '就是', '一起', '知道', '希望', '他們'], 'cluster3': ['喜歡', '真的', '一個', '這個', '還是', '時候', '自己', '一起', '大家', '看到'], 'cluster4': ['什麼', '知道', '到底', '時候', '真的', '可以', '這是', '意思', '這麼', '這個'], 'cluster5': ['不是', '有人', '怎麼', '真的', '而是', '可以', '知道', '看到', '因為', '這樣'], 'cluster6': ['自己', '一個', '覺得', '就是', '因為', '真的', '時候', '知道', '不要', '可以'], 'cluster7': ['nan', '右邊', '司機', '司法', '吃掉', '吃過', '各位', '各個', '各地', '城市']}\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","def extract_keywords_word_frequency(df, cluster_df, top_n=10):\n","    cluster_keywords = {}\n","\n","    for _, cluster in cluster_df.iterrows():\n","        cluster_members = cluster['members']\n","        cluster_texts = df[df['id'].isin(cluster_members)]['processed_text']\n","        words = ' '.join(cluster_texts).split()\n","        word_counts = Counter(words)\n","        top_keywords = [word for word, count in word_counts.most_common(top_n)]\n","        cluster_keywords[cluster['cluster_name']] = top_keywords\n","\n","    return cluster_keywords\n","\n","cluster_keywords_word_freq = extract_keywords_word_frequency(texts_df, cluster_df)\n","print(cluster_keywords_word_freq)"],"metadata":{"id":"_jIf0y2yzFIB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734835634054,"user_tz":-480,"elapsed":994,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"52b044e0-cbc8-4bfb-d0ed-0d5dce046b5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'cluster0': ['的', '我', '了', '是', '在', '你', '有', '都', '也', '好'], 'cluster1': ['真的', '的', '我', '了', '是', '好', '很', '在', '都', '你'], 'cluster2': ['的', '我', '是', '了', '在', '有', '你', '都', '也', '可以'], 'cluster3': ['的', '喜歡', '我', '你', '是', '好', '了', '很', '人', '都'], 'cluster4': ['的', '什麼', '我', '是', '了', '你', '在', '為', '都', '有'], 'cluster5': ['的', '我', '不是', '是', '有人', '了', '你', '在', '有', '也'], 'cluster6': ['的', '自己', '我', '你', '是', '了', '在', '有', '都', '人'], 'cluster7': ['nan']}\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","def extract_keywords_chi2(df, cluster_df, top_n=10):\n","    vectorizer = CountVectorizer(stop_words='english')\n","    X = vectorizer.fit_transform(df['processed_text'])\n","\n","    cluster_keywords = {}\n","\n","    for _, cluster in cluster_df.iterrows():\n","        cluster_name = cluster['cluster_name']\n","        cluster_members = cluster['members']\n","\n","        y = []\n","        for idx in df['id']:\n","            if idx in cluster_members:\n","                y.append(1)  # 屬於該 cluster\n","            else:\n","                y.append(0)  # 不屬於該 cluster\n","\n","        y = np.array(y)\n","\n","        chi2_selector = SelectKBest(chi2, k=top_n)\n","        chi2_selector.fit(X, y)\n","\n","        feature_names = vectorizer.get_feature_names_out()\n","        top_keywords = [feature_names[i] for i in chi2_selector.get_support(indices=True)]\n","\n","        cluster_keywords[cluster_name] = top_keywords\n","\n","    return cluster_keywords\n","\n","cluster_keywords_chi2 = extract_keywords_chi2(texts_df, cluster_df)\n","print(cluster_keywords_chi2)"],"metadata":{"id":"oIeDhChZzHZH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734835716064,"user_tz":-480,"elapsed":80342,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"03581051-80be-4459-8f6d-ee672f89a348"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'cluster0': ['nan', '一個', '不是', '什麼', '可以', '喜歡', '大家', '我們', '真的', '自己'], 'cluster1': ['httpswaatwmexvc', '劉知珉', '好棒', '有夠', '真的', '絕了', '美短', '自己', '財神爺', '超級'], 'cluster2': ['一個', '一起', '他們', '可以', '台灣', '因為', '大家', '就是', '希望', '我們'], 'cluster3': ['che', 'dororo', 'yaaa', '三紀', '千變', '喜歡', '小久', '數碼', '萬化', '語句'], 'cluster4': ['httpswatchouttwreportsvqubfihsoqyrdarrma', 'mcdonaldstw', '什麼', '到底', '小黨', '意思', '我為', '清潔隊', '知道', '給什麼'], 'cluster5': ['不是', '依托', '大未必佳', '娜拉', '德伯格', '有人', '東亞', '神祇', '總幹事', '而是'], 'cluster6': ['一個', '事情', '人生', '他人', '別人', '因為', '生活', '自己', '自我', '覺得'], 'cluster7': ['nan', '一個', '什麼', '可以', '喜歡', '大家', '就是', '我們', '真的', '自己']}\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","\n","def extract_keywords_pmi(df, cluster_df, top_n=10):\n","    vectorizer = CountVectorizer(stop_words='english')\n","    X = vectorizer.fit_transform(df['processed_text'])\n","\n","    term_freq = np.array(X.sum(axis=0)).flatten()  # 這是所有文檔中每個詞語的頻次\n","    total_terms = term_freq.sum()  # 所有詞語的總和\n","\n","    cluster_keywords = {}\n","    for _, cluster in cluster_df.iterrows():\n","        cluster_members = cluster['members']\n","        cluster_texts = df[df['id'].isin(cluster_members)]['processed_text']\n","        cluster_X = vectorizer.transform(cluster_texts)  # 計算每個 cluster 的詞頻\n","        term_freq_cluster = np.array(cluster_X.sum(axis=0)).flatten()\n","\n","        pmi_scores = []\n","        for i, term in enumerate(vectorizer.get_feature_names_out()):\n","            # 計算 PMI，避免除以0的情況\n","            if term_freq[i] > 0 and term_freq_cluster[i] > 0:\n","                pmi = np.log((term_freq_cluster[i] * total_terms) / (term_freq[i] * X.shape[0]))\n","            else:\n","                pmi = 0  # 如果有 0 會給 PMI 設為 0 或 np.nan\n","            pmi_scores.append((term, pmi))\n","\n","        # 根據 PMI 排序，選出 top_n 的關鍵字\n","        sorted_pmi = sorted(pmi_scores, key=lambda x: x[1], reverse=True)\n","        top_keywords = [term for term, _ in sorted_pmi[:top_n]]\n","        cluster_keywords[cluster['cluster_name']] = top_keywords\n","\n","    return cluster_keywords\n","\n","cluster_keywords_pmi = extract_keywords_pmi(texts_df, cluster_df)\n","print(cluster_keywords_pmi)"],"metadata":{"id":"Wiuw0oSkzIya","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734835722355,"user_tz":-480,"elapsed":6295,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"c4dce4bc-c51c-472c-a38c-bb100d35e5cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'cluster0': ['______________________', '____________________________', 'aalison', 'aaliyah', 'aam', 'aardman', 'abcd', 'abcde', 'abcdlov', 'abcdlove'], 'cluster1': ['ablcubrysiusvzjytsllmgoob', 'allrisepimver', 'anyways', 'astros', 'banilaco', 'bape', 'bbtaocip', 'boiip', 'bounce', 'boxlogo'], 'cluster2': ['_______________________________________', 'aaaa', 'abajo', 'abnorm', 'abnormal', 'abrazo', 'acetylcysteine', 'achil', 'achilles', 'actndaa'], 'cluster3': ['akasakibunny', 'amorous', 'anakin', 'applevaundy', 'atgwxeecurkfaarjq', 'badale', 'badalee', 'bbin', 'besot', 'besotted'], 'cluster4': ['ab型', 'aespanext', 'ay', 'baebae', 'barcode', 'beark', 'bndsakuraaaa', 'bobbi', 'chianglulu', 'chili'], 'cluster5': ['allegi', 'allegiant', 'annnbrowni', 'annnbrownie', 'basse', 'bdm', 'beau', 'blackbrown', 'bojack', 'climbs'], 'cluster6': ['adelesomeon', 'adelesomeone', 'aekoopa', 'agus', 'airtist', 'algernon', 'allwillbewel', 'allwillbewell', 'ambitiouspassion', 'angelababy'], 'cluster7': ['nan', '__', '___', '____', '_____', '______', '_______', '________', '____________', '______________________']}\n"]}]},{"cell_type":"code","source":["def save_cluster_keywords_to_csv(cluster_keywords, file_path):\n","    with open(file_path, 'w', newline='', encoding='utf-8') as f:\n","        writer = csv.writer(f)\n","        writer.writerow(['cluster_name', 'keywords'])\n","        for cluster_name, keywords in cluster_keywords.items():\n","            writer.writerow([cluster_name, ', '.join(keywords)])\n","\n","save_cluster_keywords_to_csv(cluster_keywords_tfidf, '/content/drive/MyDrive/IRTM_final_project/result/Kmeans/cluster_keywords_tfidf.csv')\n","save_cluster_keywords_to_csv(cluster_keywords_word_freq, '/content/drive/MyDrive/IRTM_final_project/result/Kmeans/cluster_keywords_word_freq.csv')\n","save_cluster_keywords_to_csv(cluster_keywords_chi2, '/content/drive/MyDrive/IRTM_final_project/result/Kmeans/cluster_keywords_chi2.csv')\n","save_cluster_keywords_to_csv(cluster_keywords_pmi, '/content/drive/MyDrive/IRTM_final_project/result/Kmeans/cluster_keywords_pmi.csv')"],"metadata":{"id":"_xI_oXC7zLW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","def count_keyword_occurrences(cluster_keywords, texts_df, cluster_df):\n","    keyword_counts = {}\n","\n","    for cluster_name, keywords in cluster_keywords.items():\n","        cluster_members = cluster_df[cluster_df['cluster_name'] == cluster_name]['members'].iloc[0]\n","        cluster_texts = texts_df[texts_df['id'].isin(cluster_members)]['text']\n","\n","        keyword_count = Counter()\n","        for text in cluster_texts:\n","          if not isinstance(text, str):\n","              continue\n","          for keyword in keywords:\n","              keyword_count[keyword] += text.count(keyword)\n","\n","        keyword_counts[cluster_name] = dict(keyword_count)\n","\n","    return keyword_counts\n","\n","# # 使用方法\n","keyword_occurrences_tfidf = count_keyword_occurrences(cluster_keywords_tfidf, texts_df, cluster_df)\n","keyword_occurrences_freq = count_keyword_occurrences(cluster_keywords_word_freq, texts_df, cluster_df)\n","keyword_occurrences_chi2 = count_keyword_occurrences(cluster_keywords_chi2, texts_df, cluster_df)\n","keyword_occurrences_pmi = count_keyword_occurrences(cluster_keywords_pmi, texts_df, cluster_df)\n","print(keyword_occurrences_pmi)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MUzhWWQXUeQ","executionInfo":{"status":"ok","timestamp":1734838102392,"user_tz":-480,"elapsed":469,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"205b331d-d6a7-43f0-c99f-25b3af9bff76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'cluster0': {'的': 29166, '我': 12294, '了': 9941, '是': 11390, '在': 7355, '你': 5776, '有': 8520, '都': 3831, '也': 3258, '好': 7619}, 'cluster1': {'真的': 3571, '的': 6674, '我': 2220, '了': 1627, '是': 1762, '好': 1667, '很': 1066, '在': 860, '都': 653, '你': 689}, 'cluster2': {'的': 38490, '我': 18042, '是': 17398, '了': 9846, '在': 9801, '有': 11856, '你': 6432, '都': 5480, '也': 5124, '可以': 4358}, 'cluster3': {'的': 2688, '喜歡': 1742, '我': 1442, '你': 689, '是': 1044, '好': 866, '了': 541, '很': 565, '人': 646, '都': 369}, 'cluster4': {'的': 2450, '什麼': 2024, '我': 1740, '是': 1543, '了': 891, '你': 836, '在': 819, '為': 821, '都': 519, '有': 922}, 'cluster5': {'的': 2506, '我': 1357, '不是': 983, '是': 2226, '有人': 727, '了': 725, '你': 676, '在': 590, '有': 1423, '也': 379}, 'cluster6': {'的': 11762, '自己': 4726, '我': 4359, '你': 3018, '是': 5021, '了': 2460, '在': 2480, '有': 3251, '都': 1564, '人': 3505}, 'cluster7': {}}\n"]}]},{"cell_type":"code","source":["from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"],"metadata":{"id":"go3ROS4hTXIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","# 字频统计字典\n","word_freq = {'的': 11762, '自己': 4726, '我': 4359, '你': 3018, '是': 5021, '了': 2460, '在': 2480, '有': 3251, '都': 1564, '人': 3505}\n","\n","# 生成文字云\n","wordcloud = WordCloud(\n","    font_path='simhei.ttf',  # 如果生成中文字云，确保指定支持中文的字体文件\n","    width=800,\n","    height=400,\n","    background_color='white',\n","    max_words=100,\n",").generate_from_frequencies(word_freq)\n","\n","# 显示文字云\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')  # 关闭坐标轴\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"6kIvS9XrTaYe","executionInfo":{"status":"error","timestamp":1734838404012,"user_tz":-480,"elapsed":1016,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"49769a63-572f-4c28-bab1-9f42fb715737"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"cannot open resource","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-632b27cdd4cd>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m ).generate_from_frequencies(word_freq)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 显示文字云\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[1;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[1;32m    455\u001b[0m                 \u001b[0;31m# find font sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    504\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;31m# try to find a position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruetype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfont_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# transpose font optionally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                 transposed_font = ImageFont.TransposedFont(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36mtruetype\u001b[0;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfreetype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36mfreetype\u001b[0;34m(font)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfreetype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStrOrBytesPath\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBinaryIO\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFreeTypeFont\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFreeTypeFont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    282\u001b[0m                         \u001b[0mload_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             self.font = core.getfont(\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0mfont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayout_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             )\n","\u001b[0;31mOSError\u001b[0m: cannot open resource"]}]},{"cell_type":"code","source":["import os\n","\n","font_path = 'C:/Windows/Fonts/kaiu.ttf'  # 替换为你的实际字体路径\n","if os.path.exists(font_path):\n","    print(\"Font file exists!\")\n","else:\n","    print(\"Font file not found. Check the path.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtERaGVwcZjR","executionInfo":{"status":"ok","timestamp":1734838794783,"user_tz":-480,"elapsed":430,"user":{"displayName":"睿宸","userId":"06895442441909130624"}},"outputId":"fe6dc1f4-ee61-43ee-d5c9-8168f91386f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Font file not found. Check the path.\n"]}]}]}