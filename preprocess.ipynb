{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Lx1oc6Mc9crV"},"outputs":[],"source":["import os\n","import re\n","import jieba\n","import pandas as pd\n","from datetime import datetime, timezone\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.corpus import stopwords"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_X0baKtaBs8d","executionInfo":{"status":"ok","timestamp":1734684792163,"user_tz":-480,"elapsed":2157,"user":{"displayName":"learn","userId":"14094276025331966874"}},"outputId":"7927a69f-4be9-4711-d43e-02256ebe9deb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUmFJMj2A6EM","executionInfo":{"status":"ok","timestamp":1734684792164,"user_tz":-480,"elapsed":8,"user":{"displayName":"learn","userId":"14094276025331966874"}},"outputId":"3da06228-2813-4f0b-8e93-7bf4fca9e1bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["input_file = '/content/drive/My Drive/IRTM_final_project/data/posts/1220.csv'\n","output_file = '/content/drive/My Drive/IRTM_final_project/vectors/1220_vector.csv'"],"metadata":{"id":"KyTAv5-0puj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","print(current_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqcWUXm9NS5Q","executionInfo":{"status":"ok","timestamp":1734684792164,"user_tz":-480,"elapsed":7,"user":{"displayName":"learn","userId":"14094276025331966874"}},"outputId":"f37f3780-12ec-4f79-f564-d70975727a18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-12-20 08:53:11\n"]}]},{"cell_type":"code","source":["def preprocess_text(text):\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    text = re.sub(r'\\d+', '', text)  # 移除數字\n","\n","    # 使用正規表達式分詞\n","    tokens = re.findall(r'\\b\\w+\\b', text.lower())  # 切割成單字，並轉為小寫\n","    english_tokens = [stemmer.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n","\n","    # 中文處理\n","    chinese_tokens = jieba.lcut(text)\n","\n","    # 合併英文和中文的 tokens\n","    combined_tokens = english_tokens + chinese_tokens\n","    return ' '.join(combined_tokens)"],"metadata":{"id":"jzGjt8oJ90IJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_file(file, vectorizer, processed_texts):\n","    df = pd.read_csv(file)\n","\n","    df = df[['id', 'text', 'reply_count', 'like_count']]\n","\n","    # 使用緩存的 processed_text\n","    df['processed_text'] = processed_texts\n","    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(current_time)\n","\n","    tfidf_matrix = vectorizer.transform(df['processed_text'])\n","    vectors = tfidf_matrix.toarray()  # 轉換為 NumPy 陣列\n","\n","    # 將每行的向量轉為列表格式，並儲存到 'vector' 欄位\n","    df['vector'] = [list(vector) for vector in vectors]\n","\n","    # 保留 'id', 'reply_count', 'like_count' 及 'vector' 欄位\n","    result = df[['id', 'reply_count', 'like_count', 'vector']]\n","\n","    # 儲存結果至檔案\n","    result.to_csv(output_file, index=False)\n","    print(f\"前處理完成，結果儲存至 {output_file}\")"],"metadata":{"id":"8DzXEGxuIEmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 收集所有文本並處理一次\n","print(\"初始化 TF-IDF Vectorizer...\")\n","df = pd.read_csv(input_file)\n","processed_texts = df['text'].apply(preprocess_text)\n","\n","current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","print(current_time)\n","\n","# 初始化 TF-IDF Vectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # 設定最大特徵數量\n","tfidf_vectorizer.fit(processed_texts)\n","\n","current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","print(current_time)\n","\n","# 使用處理後的文本進行向量化\n","print(\"開始處理 target.csv...\")\n","process_file(input_file, vectorizer=tfidf_vectorizer, processed_texts=processed_texts)"],"metadata":{"id":"Hf4-GQhdYogE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a270a475-c9ad-45d8-c159-5fc32c2b27c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["初始化 TF-IDF Vectorizer...\n"]},{"output_type":"stream","name":"stderr","text":["Building prefix dict from the default dictionary ...\n","DEBUG:jieba:Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.689 seconds.\n","DEBUG:jieba:Loading model cost 0.689 seconds.\n","Prefix dict has been built successfully.\n","DEBUG:jieba:Prefix dict has been built successfully.\n"]},{"output_type":"stream","name":"stdout","text":["2024-12-20 08:53:20\n","2024-12-20 08:53:22\n","開始處理 target.csv...\n","2024-12-20 08:53:22\n"]}]},{"cell_type":"code","source":["current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","print(current_time)"],"metadata":{"id":"zTZTJ29DEwZf"},"execution_count":null,"outputs":[]}]}